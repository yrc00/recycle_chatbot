import os
from dotenv import load_dotenv

import streamlit as st
from typing import List
from langchain_groq import ChatGroq
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_core.documents import Document
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_cohere import CohereEmbeddings, CohereRerank
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever

######################### í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ #########################

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if GROQ_API_KEY is None:
    st.error("GROQ_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

COHERE_API_KEY = os.getenv("COHERE_API_KEY")
if COHERE_API_KEY is None:
    st.error("COHERE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

#########################  í˜ì´ì§€ ì„¤ì • #########################

st.set_page_config(page_title="Recycle Chatbot", page_icon="â™»ï¸")
st.title("â™»ï¸ ì„œìš¸ì‹œ êµ¬ë³„ ì¬í™œìš© ì±—ë´‡")

#########################  êµ¬ ì„ íƒ #########################
gu_list = ["ìš©ì‚°êµ¬"]

with st.sidebar:
    selected_gu = st.selectbox("êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”", gu_list)

######################### RAG ê²€ìƒ‰ í•¨ìˆ˜ (ì‹¤ì œ êµ¬í˜„ í•„ìš”) #########################

# PDF ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_documents(selected_gu: str):
    loaders = []
    
    # ê³µí†µ PDF íŒŒì¼ ë¡œë“œ
    if os.path.exists("./data/ê³µí†µ.pdf"):
        common_pdf_path = "./data/ê³µí†µ.pdf"
        common_loader = PyPDFLoader(common_pdf_path)
        loaders.append(common_loader)
    else:
        st.warning("ê³µí†µ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. êµ¬ë³„ PDFë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        common_loader = None
    
    # ì„ íƒëœ êµ¬ì˜ PDF íŒŒì¼ ë¡œë“œ
    gu_pdf_path = os.path.join(os.path.dirname(__file__), f"data/{selected_gu}.pdf")
    if os.path.exists(gu_pdf_path):  # í•´ë‹¹ êµ¬ì˜ PDFê°€ ìˆì„ ê²½ìš°ë§Œ ë¡œë“œ
        gu_loader = PyPDFLoader(gu_pdf_path)
        loaders.append(gu_loader)
    else:
        st.warning(f"{selected_gu}ì— ëŒ€í•œ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê³µí†µ PDFë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        gu_loader = None
        loaders.append(common_loader)
    
    all_docs = []
    for loader in loaders:
        docs = loader.load()
        all_docs.extend(docs)
    
    splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", ""],
        chunk_size=1000,
        chunk_overlap=300,
        length_function=len,
        is_separator_regex=False,
    )
    split_docs = splitter.split_documents(all_docs)
    return split_docs

# ë¬¸ì„œ ë‚´ìš©ì„ ì •ë¦¬(clean)í•˜ëŠ” í•¨ìˆ˜
def clean_documents(documents: List[Document]) -> str:
    cleaned = [
        doc.page_content.replace("\n", " ").strip()
        for doc in documents
    ]
    return "\n\n".join(cleaned)

# ì „ì²´ ì²´ì¸ ìƒì„± í•¨ìˆ˜
def get_rag_chain(split_documents, groq_api_key: str, cohere_api_key: str) -> tuple[Runnable, ContextualCompressionRetriever]:
    # 1. ê¸°ë³¸ retriever
    base_retriever = FAISS.from_documents(
        split_documents,
        CohereEmbeddings(model="embed-multilingual-v3.0", cohere_api_key=cohere_api_key)
    ).as_retriever(search_kwargs={"k": 10})

    # 2. Cohere reranker ê¸°ë°˜ compression retriever
    compressor = CohereRerank(
        model="rerank-multilingual-v3.0", 
        top_n=2, 
        cohere_api_key=cohere_api_key
    )
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    # 3. LLM (Groq)
    llm = ChatGroq(model="llama3-8b-8192", api_key=groq_api_key)

    # 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    system_prompt = (
        "You are a helper who tells the user how to dispose of garbage. "
        "Given the context, answer in Korean. "
        "If you don't know the answer, say you don't know. "
        "Context: {context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    # 5. ë¬¸ì„œ ê²°í•© ì²´ì¸
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)

    # 6. ì „ì²´ retrieval chain
    rag_chain = create_retrieval_chain(compression_retriever, combine_docs_chain)

    return rag_chain, compression_retriever

#########################  ì±—ë´‡ #########################

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.spinner("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘..."):
    docs = load_and_split_documents(selected_gu)
    qa_chain, compression_retriever = get_rag_chain(docs, GROQ_API_KEY, COHERE_API_KEY)


# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì„¸ì…˜
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

# ì´ì „ ëŒ€í™” í‘œì‹œ
for msg, resp in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(msg)
    with st.chat_message("assistant"):
        st.markdown(resp)

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.chat_input("ì¬í™œìš© ë°©ë²•ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            # ë¬¸ì„œ ì••ì¶• ìˆ˜í–‰
            compressed_docs = compression_retriever.invoke(user_input)
            context = clean_documents(compressed_docs)

            # ì²´ì¸ ì‹¤í–‰
            result = qa_chain.invoke({
                "input": user_input,
                "context": context
            })
            st.markdown(result["answer"])

            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.chat_history.append((user_input, result["answer"]))
