import streamlit as st
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage, SystemMessage
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chains import LLMChain
from dotenv import load_dotenv
from typing import List
import os

######################### í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ #########################
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if GROQ_API_KEY is None:
    st.error("GROQ_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

#########################  í˜ì´ì§€ ì„¤ì • #########################
st.set_page_config(page_title="Recycle Chatbot", page_icon="â™»ï¸")
st.title("â™»ï¸ ì„œìš¸ì‹œ êµ¬ë³„ ì¬í™œìš© ì±—ë´‡")

#########################  êµ¬ ì„ íƒ #########################
gu_list = ["ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬",
           "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬",
           "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬", "ì„±ë¶êµ¬",
           "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬",
           "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"]

with st.sidebar:
    selected_gu = st.selectbox("êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”", gu_list)

######################### RAG ê²€ìƒ‰ í•¨ìˆ˜ (ì‹¤ì œ êµ¬í˜„ í•„ìš”) #########################

# PDF ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_documents(selected_gu: str):
    loaders = []
    
    # ê³µí†µ PDF íŒŒì¼ ë¡œë“œ
    common_pdf_path = "./data/ê³µí†µ.pdf"
    common_loader = PyPDFLoader(common_pdf_path)
    loaders.append(common_loader)
    
    # ì„ íƒëœ êµ¬ì˜ PDF íŒŒì¼ ë¡œë“œ
    gu_pdf_path = f"./data/{selected_gu}.pdf"
    if os.path.exists(gu_pdf_path):  # í•´ë‹¹ êµ¬ì˜ PDFê°€ ìˆì„ ê²½ìš°ë§Œ ë¡œë“œ
        gu_loader = PyPDFLoader(gu_pdf_path)
        loaders.append(gu_loader)
    
    all_docs = []
    for loader in loaders:
        docs = loader.load()
        all_docs.extend(docs)
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(all_docs)
    return split_docs

# ì„ë² ë”© ë° ë²¡í„° DB ìƒì„±
@st.cache_resource
def create_vectorstore(_docs):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return FAISS.from_documents(_docs, embeddings)


# Groq ê¸°ë°˜ RAG ì²´ì¸ ìƒì„±
def get_rag_chain(vectorstore, groq_api_key):
    retriever = vectorstore.as_retriever()

    # Groq LLM ì„¤ì •
    llm = ChatGroq(api_key=groq_api_key, model_name="gemma2-9b-it")  # ìµœì‹  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        ë„ˆëŠ” ì„œìš¸ì‹œì˜ ì¬í™œìš© ì •ì±…ì„ ì˜ ì•„ëŠ” ì•ˆë‚´ ì±—ë´‡ì´ì•¼.\n
        ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•´ì¤˜.\n

        [ë¬¸ì„œ ìš”ì•½]
        {context}

        [ì§ˆë¬¸]
        {question}

        [ë‹µë³€]
        """,
    )

    # RetrievalQA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    return qa_chain

def retrieve_documents(query: str) -> List[str]:
    # ì—¬ê¸°ì— FAISS, Chroma ë“±ì—ì„œ ê²€ìƒ‰í•˜ì—¬ ìœ ì‚¬í•œ ì •ì±… ë¬¸ì„œ ë°˜í™˜
    # ì˜ˆì‹œ:
    return [
        "ê°•ë‚¨êµ¬ëŠ” íˆ¬ëª… í˜íŠ¸ë³‘ì„ ë³„ë„ë¡œ ë¶„ë¦¬ ë°°ì¶œí•´ì•¼ í•˜ë©°...",
        "í”Œë¼ìŠ¤í‹±ì€ ë¹„ë‹ê³¼ ë¶„ë¦¬í•˜ì—¬ ë°°ì¶œí•´ì•¼ í•©ë‹ˆë‹¤..."
    ]

#########################  ì±—ë´‡ #########################
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.spinner("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘..."):
    docs = load_and_split_documents(selected_gu)
    vectorstore = create_vectorstore(docs)
    qa_chain = get_rag_chain(vectorstore, GROQ_API_KEY)

# ì±„íŒ… ì…ë ¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.chat_input("ì¬í™œìš© ë°©ë²•ì„ ì§ˆë¬¸í•˜ì„¸ìš”.")
if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        response = qa_chain.invoke({"query": user_input})
        st.markdown(response["result"])

    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    st.session_state.chat_history.append((user_input, response["result"]))
